/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:43107'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:33269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:33202'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:38315'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:35299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:34614'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:41301'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:36179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:40766'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:41949'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:33914'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:46566'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:35568'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:39268'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:39704'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:42026'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:44701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:42211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:39432'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:36580'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:40992'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:40722'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:38156'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:43174'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:34093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:40057'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:37587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.126:36185'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d7eud4km', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ga54bu4h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k1hs7oj5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ysjr3cof', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0pfd1buh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4j9j5wg3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_25c0v2r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4qdcqzvw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0kj46vey', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-meugx4j1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3p_q29lw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-63f0tpzp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xi9vgthp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b6bau9_j', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-j8embt1b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-aa89ul8z', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rpjn1qxz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1es84co8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5i_q32yd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dy077pt9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-e0xxxean', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sk6rpe94', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-c56zro_7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-s7swvav3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ggw5t8gn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6_vizhw4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-66h90xgs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gtd_2fxg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rbw0p5v6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ua5vkp_8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2giuw1jo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ot8ynwaf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-juefxtvx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3tqpcfr8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pdz_vmd5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pxbqlry6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-s53k9v69', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bqpjbdr2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-05ryl5kp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-325qhpfw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6_elwsa4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dshxm413', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jxhgp6_n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2_yyloc8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sua8o3iv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-14_llcxb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9kw33e3m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-v2zhg8te', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4r0cq1yi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fe51v9qw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ccxmjx8b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oi44u9c0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1q8hdajh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5jhuu7d0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tjzqxxv_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7q2_frnw', purging
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:36594
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:35451
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:37907
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:36594
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:34253
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:34213
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:46257
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:35451
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:37907
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:35493
distributed.worker - INFO -          dashboard at:         172.30.8.126:36327
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:34253
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:36735
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:46257
distributed.worker - INFO -          dashboard at:         172.30.8.126:44315
distributed.worker - INFO -          dashboard at:         172.30.8.126:40263
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:40886
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:35493
distributed.worker - INFO -          dashboard at:         172.30.8.126:36763
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:36735
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:34213
distributed.worker - INFO -          dashboard at:         172.30.8.126:43501
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:40886
distributed.worker - INFO -          dashboard at:         172.30.8.126:34864
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:37872
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          dashboard at:         172.30.8.126:41662
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:33964
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:38210
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:46816
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:43831
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:37567
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:40637
distributed.worker - INFO -          dashboard at:         172.30.8.126:39087
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:40772
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:34694
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:40830
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:33942
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:37872
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:38210
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:46816
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:33964
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:43831
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:37567
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:40637
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:40830
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:40772
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:33942
distributed.worker - INFO -          dashboard at:         172.30.8.126:37863
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.30.8.126:45941
distributed.worker - INFO -          dashboard at:         172.30.8.126:38873
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-z7c2ucjx
distributed.worker - INFO -          dashboard at:         172.30.8.126:35860
distributed.worker - INFO -          dashboard at:         172.30.8.126:38870
distributed.worker - INFO -          dashboard at:         172.30.8.126:45874
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:34694
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-467j8j0p
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:34399
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.8.126:39325
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:43208
distributed.worker - INFO -          dashboard at:         172.30.8.126:41572
distributed.worker - INFO -          dashboard at:         172.30.8.126:35084
distributed.worker - INFO -          dashboard at:         172.30.8.126:34857
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:         172.30.8.126:44611
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:34399
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.8.126:44628
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:45105
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:43208
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-54in_ukt
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-72qviir2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.8.126:34194
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4mqk2gs9
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:45105
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.8.126:33527
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.8.126:39517
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:35174
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vyur66bi
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vkxr1giz
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:35174
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uu0ajz34
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:44758
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_630bsrl
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ocdymetf
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xypmkcu0
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0cu79igj
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-un7empv4
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:46359
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-57m3z83a
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_84wn2zw
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tmgwjws0
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x9wad4c6
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uf_3mmyc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:46359
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4msntiw9
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:44758
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mzjpixq3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-si8n6c7m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:         172.30.8.126:33688
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:39125
distributed.worker - INFO -          dashboard at:         172.30.8.126:46408
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.8.126:43175
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fkoy_s0w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:39125
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.126:34302
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_75gab5_
distributed.worker - INFO -          dashboard at:         172.30.8.126:40797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.8.126:34302
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.8.126:41809
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7yvnm4h4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1mype23b
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rz4tm6_f
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k3xt731s
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h8om0qxw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.
OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.
OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.
distributed.utils_perf - INFO - full garbage collection released 19.70 MB from 264 reference cycles (threshold: 10.00 MB)
OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.
distributed.utils_perf - INFO - full garbage collection released 11.78 MB from 1125 reference cycles (threshold: 10.00 MB)
OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.
OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:60046 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:60048 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:60050 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 43.98 MB from 1017 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 135.93 MB from 785 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 23.83 MB from 1658 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 131.90 MB from 1361 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33070 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33078 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 67.79 MB from 338 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33080 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33086 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33062 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33068 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33072 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33076 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33082 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 27.73 MB from 1204 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33090 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33074 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 22.88 MB from 1332 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 16.54 MB from 1606 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 91.31 MB from 1738 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 25.58 MB from 506 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 75.80 MB from 1235 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 108.72 MB from 1811 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 44.62 MB from 630 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 20.13 MB from 1881 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 23.85 MB from 1735 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 37.70 MB from 1242 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 40.09 MB from 1892 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.77 MB from 993 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 31.60 MB from 4219 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 43.74 MB from 2161 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.67 MB from 1389 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.83 MB from 1239 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 143.71 MB from 624 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 23.85 MB from 990 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 24.04 MB from 1959 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 18.30 MB from 3656 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 28.00 MB from 1474 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 23.84 MB from 1232 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33510 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33602 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33612 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33618 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33586 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33418 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33698 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33522 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33410 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33604 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33436 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33606 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33734 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33440 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33518 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33508 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33406 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33620 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33746 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33596 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33540 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33592 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33590 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33422 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33424 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33584 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33416 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33512 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33624 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33588 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33514 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33402 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33742 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33520 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33408 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33526 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33582 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33414 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33598 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33430 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33740 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33594 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33650 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33426 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33738 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33444 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 23.94 MB from 2262 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33464 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33470 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33474 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33530 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33642 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33492 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33548 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33660 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33456 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33736 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33554 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33666 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 127.82 MB from 2613 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33728 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33434 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33490 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33546 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33658 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33652 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33608 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33732 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33438 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33496 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33552 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33662 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33516 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33404 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33460 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33628 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33462 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33630 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33478 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33534 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33646 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33486 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33542 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33622 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33748 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33454 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33466 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33634 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33452 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33494 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33550 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33664 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33524 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33412 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33468 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33636 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33458 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33626 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33614 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33446 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33502 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33558 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33670 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33744 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33450 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33506 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33488 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33544 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33656 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33482 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33538 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33528 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33640 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33500 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33556 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33668 remote=tcp://172.30.100.3:33470>
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33420 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33476 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33532 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33644 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33638 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33632 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 20.22 MB from 1032 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33480 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.126:33648 remote=tcp://172.30.100.3:33470>
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b92f8e69d68>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b92f8e69d68>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b92f8e69d68>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b92f8e69d68>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b92f8e69d68>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b92f8e69d68>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.utils_perf - INFO - full garbage collection released 17.32 MB from 3322 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:44758
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:46359
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:36735
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:40992'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:33942
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:34399
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:40637
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:38210
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:35451
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:37907
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:33964
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:45105
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:34213
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:46816
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:35568'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:40722'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:43208
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:33914'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:38156'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:40057'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:42211'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:35299'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:34093'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:43107'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:33269'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:33202'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:36185'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:41301'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:46257
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:36179'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:36594
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:46566'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:40830
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:34694
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:41949'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:44701'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:37567
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:35174
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:34302
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:42026'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:34614'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:39268'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.8.126:40772
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.126:43174'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.dask_worker - INFO - End worker
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
