/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:37116'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:39499'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:35126'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:33769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:35266'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:38243'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:41186'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:39612'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:41328'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:35975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:40342'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:39898'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:34288'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:46155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:44636'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:38366'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:44241'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:45176'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:36379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:38886'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:39912'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:44651'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:33972'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:38919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:36808'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:45108'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:35813'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.84:38013'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:34572
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:39854
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:34572
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:35317
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:37490
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:39854
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:36285
distributed.worker - INFO -          dashboard at:          172.30.5.84:35814
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:37786
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:35317
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:39825
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:43519
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:37490
distributed.worker - INFO -          dashboard at:          172.30.5.84:36462
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:36285
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:37786
distributed.worker - INFO -          dashboard at:          172.30.5.84:33147
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:39825
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:43519
distributed.worker - INFO -          dashboard at:          172.30.5.84:39496
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.30.5.84:37354
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          dashboard at:          172.30.5.84:35599
distributed.worker - INFO -          dashboard at:          172.30.5.84:35077
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          dashboard at:          172.30.5.84:45904
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:33010
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:36479
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:35012
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:42902
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:34059
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:33010
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:36479
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:35012
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:          172.30.5.84:38991
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:34059
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:42902
distributed.worker - INFO -          dashboard at:          172.30.5.84:35964
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-txw0atpj
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:          172.30.5.84:41941
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2rhmssfi
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4avyd9k6
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fqzp2epk
distributed.worker - INFO -          dashboard at:          172.30.5.84:38046
distributed.worker - INFO -          dashboard at:          172.30.5.84:33510
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-y63rzl7q
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vughf0s_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-94l64fn1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9smymb2x
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fht_7897
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8nt3ev85
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-q4_rtzje
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vsgiec00
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-o_juitc2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:38839
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:38839
distributed.worker - INFO -          dashboard at:          172.30.5.84:46231
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1kfa4vyt
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:41102
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:41102
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:40449
distributed.worker - INFO -          dashboard at:          172.30.5.84:45034
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:40449
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.30.5.84:33007
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-s1t75nux
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pk7lth9e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:37345
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:37345
distributed.worker - INFO -          dashboard at:          172.30.5.84:37991
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yb1npsdt
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:45447
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:45447
distributed.worker - INFO -          dashboard at:          172.30.5.84:44453
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2oxga71f
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:39772
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:35215
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:39772
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:35215
distributed.worker - INFO -          dashboard at:          172.30.5.84:34308
distributed.worker - INFO -          dashboard at:          172.30.5.84:41148
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:35234
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:35234
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:          172.30.5.84:42297
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-foz3fs7e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yexepwg_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6skxmhds
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:43959
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:43959
distributed.worker - INFO -          dashboard at:          172.30.5.84:41090
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:42281
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:42281
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:          172.30.5.84:40497
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7p_60co2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gqe0y1y5
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:42025
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:42025
distributed.worker - INFO -          dashboard at:          172.30.5.84:35270
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-n_m3zfis
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:35593
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:35593
distributed.worker - INFO -          dashboard at:          172.30.5.84:45280
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-710w52lc
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:44506
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:44506
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.30.5.84:38226
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:46333
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:46333
distributed.worker - INFO -          dashboard at:          172.30.5.84:39426
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jwgdn58s
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2a9_o0bv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.5.84:33216
distributed.worker - INFO -          Listening to:    tcp://172.30.5.84:33216
distributed.worker - INFO -          dashboard at:          172.30.5.84:35050
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rrwr2gup
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33470
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.
OMP: Warning #190: Forking a process while a parallel region is active is potentially unsafe.
distributed.utils_perf - INFO - full garbage collection released 11.69 MB from 1291 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 11.62 MB from 830 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 17.87 MB from 936 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:60930 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:60932 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 45.79 MB from 1462 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 99.98 MB from 1267 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 47.98 MB from 1580 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 79.91 MB from 1609 reference cycles (threshold: 10.00 MB)
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.3:33470' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 791, in heartbeat
    address=self.contact_address, now=time(), metrics=self.get_metrics()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 738, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 866, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.3:33470' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35560 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35528 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35584 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35550 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35554 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35586 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35572 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35564 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35526 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35578 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35562 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35568 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35558 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35556 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35530 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:35588 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 22.52 MB from 2120 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 119.34 MB from 1194 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 219.38 MB from 1599 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 89.25 MB from 2857 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 39.68 MB from 633 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 157.22 MB from 1293 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 17.29 MB from 1321 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 17.35 MB from 1369 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 20.15 MB from 1590 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 23.78 MB from 1969 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 71.70 MB from 1547 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 43.87 MB from 2452 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.60 MB from 1577 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.59 MB from 1459 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 51.67 MB from 1661 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.85 MB from 1358 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 18.38 MB from 1967 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.74 MB from 1454 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.85 MB from 1739 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 16.00 MB from 2317 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36118 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36126 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36105 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36112 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36210 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36200 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36260 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36032 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36120 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36008 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36038 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36202 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36206 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36034 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36262 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36116 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36004 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36128 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36188 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36020 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36122 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36010 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36016 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36110 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36214 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36332 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36046 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36074 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36130 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36186 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36018 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36212 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36330 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36044 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36190 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36022 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36196 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36028 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36198 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36030 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 64.00 MB from 1058 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36106 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36108 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 43.85 MB from 1815 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36204 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36208 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36096 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36152 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36094 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36150 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36266 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36050 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36218 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36102 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36078 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36134 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36246 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36306 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36066 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36234 remote=tcp://172.30.100.3:33470>
distributed.utils_perf - INFO - full garbage collection released 39.73 MB from 1976 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36192 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36024 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36080 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36136 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36248 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36308 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36006 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36062 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36230 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36064 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36232 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36216 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36048 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36124 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36012 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36068 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36236 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36144 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36014 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36070 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36238 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36100 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36090 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36146 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36036 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36092 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36148 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36264 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36114 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36002 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36058 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36226 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36194 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36026 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36082 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36138 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36250 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36310 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36054 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36224 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36076 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36132 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36244 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36304 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36072 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36240 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36242 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36328 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36042 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36098 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36154 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36270 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36086 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36142 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36254 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36056 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36222 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36052 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36220 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36060 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36228 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36084 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36140 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36252 remote=tcp://172.30.100.3:33470>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36312 remote=tcp://172.30.100.3:33470>
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.utils_perf - INFO - full garbage collection released 20.25 MB from 876 reference cycles (threshold: 10.00 MB)
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.84:36268 remote=tcp://172.30.100.3:33470>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b8bad5b20f0>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33470' processes=112 cores=112>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.utils_perf - INFO - full garbage collection released 20.59 MB from 4297 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:35215
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:37786
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:38839
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:35317
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:39854
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:35012
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:43519
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:37490
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:33010
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:39825
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:33769'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:35126'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:36808'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:38919'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:36379'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:45108'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:39612'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:44636'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:46155'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:38366'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33470'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.84:40449
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.84:38243'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33470
distributed.dask_worker - INFO - End worker
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
