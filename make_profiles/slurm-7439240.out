/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:46120'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:32801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:33791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:39046'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:39419'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:45708'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:46114'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:37515'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:42814'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:42725'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:38400'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:36385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:39559'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:41839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:45954'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:37044'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:38845'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:45463'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:42893'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:35013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:42860'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:36461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:34745'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:37918'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:33844'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:33074'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:43328'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.5.100:42879'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6nawcchm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2cujhzn_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b2kpfjns', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-06s3e0nn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9hj76vts', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oed0vzhn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h4absy7o', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0yo7rn5a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-plpgqj6_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1lq8fj7n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wtwnoa54', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ab5q4ads', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nmv73mra', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sk4unk0t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-tcuik5yz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_we8l9sy', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bggcrb1g', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mu0o6_pg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-i29iink3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hodo7mys', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5te3gskg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6epq4xx1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qk47tf91', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nkalqam3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qn7icwer', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xeixhbzy', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dpmnh1v5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3qoi4dw5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-l8pvw72l', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1vfn644k', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-y1qrj8jo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-eu9zykem', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zervnd1j', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8wm0u773', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ooju4iqm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6xr1ar5v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w3p2lawk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-78nixhsg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hgnz093w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_31qzr1a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-n1gm___c', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rpo117ty', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6wweaebj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-z4gp8r0m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fbf0e4c_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vqk4umam', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-a8p5u7o2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4051kxdk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3pxua5aq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ngnjebpq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-v21uty2m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_7t5sou2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-utflyulh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-a183noal', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qt6rkvwl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xn1fo6sa', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-m0ps2izk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pnoy_ffm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k3h0hlg1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b4xlv864', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yd7gdji9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uquzdx5j', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9n7heltc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d6p92x_v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yd3et9xy', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gsv3gxrx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-4e83j8od', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-en5de9m9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5ykcrrqj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f5m_wi80', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-v3pi306q', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-469yy3kj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3pb22b1b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g31uex7y', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-r8lwaevj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qmwsbmuz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3safd29z', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x6i2v8ak', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vffrr3pe', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8zre8dko', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2o1dljfc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pdsoyrp3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0bdm598k', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fdt1k3es', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9ml53mev', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5qr_1hur', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dkz5tka5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-akiyt6bj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_me6itrh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-36e79dk2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pd80dz00', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3xt3w08r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nvq43mx7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ltl4_gf_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kcd3f8_e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_j8vyntm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-km9rsm9z', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t5v6o6hf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ig7b5rr5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bpgi1968', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ubbnvf3q', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6_1biskm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0r4gbzlq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g2v0spfa', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-i6l3zan5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ntwrsye4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fbuh_nw8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jg0hvwbj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-238w92lv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mq24dz9j', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-834jm8bm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nk1z8515', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hflbe0n0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gqtdozs3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mmowoufk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yhtgbwf9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jw5ch4bx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ojcgc1co', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1wb08818', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nk1iyckf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f7kimbzv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dv9fo3g4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-h3bfu7ue', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dqqdc6mw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-a1qiqwyo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-unf8kugp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7xp69pxv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3goqxiou', purging
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:45656
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:45656
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:42779
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:44887
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:32783
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:38451
distributed.worker - INFO -          dashboard at:         172.30.5.100:36754
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:35537
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:46437
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:36768
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:42779
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:44887
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:32783
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:38451
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:35829
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:35537
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:46437
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:36768
distributed.worker - INFO -          dashboard at:         172.30.5.100:44080
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.5.100:33872
distributed.worker - INFO -          dashboard at:         172.30.5.100:45824
distributed.worker - INFO -          dashboard at:         172.30.5.100:42733
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:35829
distributed.worker - INFO -          dashboard at:         172.30.5.100:35210
distributed.worker - INFO -          dashboard at:         172.30.5.100:38620
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -          dashboard at:         172.30.5.100:36521
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:42054
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:40090
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:34130
distributed.worker - INFO -          dashboard at:         172.30.5.100:35973
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:40090
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:42054
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:45672
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:37749
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:34130
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:35991
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:44131
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.30.5.100:37230
distributed.worker - INFO -          dashboard at:         172.30.5.100:33523
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xge7zv69
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:37749
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:45672
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.30.5.100:38153
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:35991
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:44131
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qmb6xjgb
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fpefii0u
distributed.worker - INFO -          dashboard at:         172.30.5.100:45918
distributed.worker - INFO -          dashboard at:         172.30.5.100:39308
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-f2qm3y5h
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sdo3hmzs
distributed.worker - INFO -          dashboard at:         172.30.5.100:34365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.5.100:34270
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xu4irc7r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8t2_qxys
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mu20eyu5
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-c12545le
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6sbmybtu
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oesqc1xd
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_fghjgbe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9gqh7ylm
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rwuiyd30
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5c8q2sah
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-j4zjii6p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:32819
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:32819
distributed.worker - INFO -          dashboard at:         172.30.5.100:43711
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:42640
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:42640
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.5.100:41639
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zhibfe4q
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qo4wp_sd
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:37394
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:37394
distributed.worker - INFO -          dashboard at:         172.30.5.100:36664
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:37125
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:37125
distributed.worker - INFO -          dashboard at:         172.30.5.100:34422
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-eatexmby
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yu2b10eh
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:34730
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:34730
distributed.worker - INFO -          dashboard at:         172.30.5.100:42564
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:38749
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:41690
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:38749
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:41690
distributed.worker - INFO -          dashboard at:         172.30.5.100:33034
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:40609
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:         172.30.5.100:34247
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ppi6y9bm
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:40609
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -          dashboard at:         172.30.5.100:41645
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ckxn4h_l
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-66nhtil6
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-x864quv5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:33917
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:36170
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:33917
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:36170
distributed.worker - INFO -          dashboard at:         172.30.5.100:44803
distributed.worker - INFO -          dashboard at:         172.30.5.100:37100
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-p69cojgz
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bdzo85y4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:40320
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.100:42607
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:40320
distributed.worker - INFO -          Listening to:   tcp://172.30.5.100:42607
distributed.worker - INFO -          dashboard at:         172.30.5.100:39870
distributed.worker - INFO -          dashboard at:         172.30.5.100:34079
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8imc58cw
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yc7r4w09
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:33845
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 43.73 MB from 243 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 17.25 MB from 837 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 17.34 MB from 648 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58042 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58098 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58052 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57956 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57992 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58048 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58160 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58028 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58084 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57958 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58070 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57982 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58150 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58152 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58064 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58120 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58176 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57994 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58050 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58106 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58020 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58076 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58188 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57954 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57988 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58100 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57962 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58016 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58072 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58022 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58078 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58026 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58080 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58190 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58006 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58062 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58118 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 43.59 MB from 2194 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 62.24 MB from 749 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57952 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58196 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57972 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 19.33 MB from 789 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57964 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 23.29 MB from 1014 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57998 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58110 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58166 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 15.47 MB from 1103 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58200 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57976 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57960 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58074 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58034 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58146 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57978 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58030 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58086 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58198 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57974 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57950 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58036 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58092 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58148 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58204 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57980 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58024 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58082 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58192 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57968 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 13.77 MB from 1573 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58058 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58164 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57990 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58046 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57966 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 14.64 MB from 1256 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:57970 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58116 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:58172 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 23.78 MB from 1642 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 127.79 MB from 828 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 59.81 MB from 1509 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 51.76 MB from 1421 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 14.72 MB from 2388 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 71.98 MB from 1265 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 16.75 MB from 1108 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 23.72 MB from 1641 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33268 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33278 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33260 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33228 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33284 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33264 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33272 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33230 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33286 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33270 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33274 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33282 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33258 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33266 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33232 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33288 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33256 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33262 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33276 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 27.63 MB from 887 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 16.46 MB from 2124 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33252 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 20.68 MB from 3162 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33280 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 16.54 MB from 2810 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33344 remote=tcp://172.30.100.3:33845>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.5.100:33254 remote=tcp://172.30.100.3:33845>
distributed.utils_perf - INFO - full garbage collection released 72.42 MB from 1857 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 41.45 MB from 1011 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 18.31 MB from 1512 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 20.26 MB from 1432 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 23.69 MB from 2661 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.62 MB from 1686 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 27.77 MB from 1430 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.84 MB from 6239 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 27.60 MB from 1319 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 23.84 MB from 1622 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.84 MB from 3335 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 19.72 MB from 3297 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 40.41 MB from 2054 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Comm closed
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.5.99:37595
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1890, in gather_dep
    {"op": "add-keys", "keys": list(response["data"])}
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - ERROR - 
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1423, in transition_dep_flight_memory
    self.batched_stream.send({"op": "add-keys", "keys": [dep]})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b4a9c994198>>, <Future finished exception=CommClosedError()>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.send(value)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1914, in gather_dep
    self.transition_dep(d, "memory", value=data[d])
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1355, in transition_dep
    state = func(dep, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1423, in transition_dep_flight_memory
    self.batched_stream.send({"op": "add-keys", "keys": [dep]})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - INFO - Comm closed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b4a9c994198>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b4a9c994198>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b4a9c994198>>, <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed')>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1940, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2011, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 741, in send_recv_from_rpc
    result = yield send_recv(comm=comm, op=key, **kwargs)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.utils_perf - INFO - full garbage collection released 18.33 MB from 1319 reference cycles (threshold: 10.00 MB)
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.3:33845' processes=140 cores=140>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1077, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 3.00 seconds, closing client
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.utils_perf - INFO - full garbage collection released 187.70 MB from 4607 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33845'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.100:41690
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.100:39419'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33845'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.100:42054
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.100:35013'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.3:33845'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.5.100:44131
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.5.100:46114'
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:33845
distributed.dask_worker - INFO - End worker
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
